Training and tuning :This app loads your trained weights (best.pt) for inference, so any changes to epochs, batch size, or image size (imgsz) are made during training and then reflected in the app once you point it to the new best.pt. For a fast, reliable baseline, start with YOLO11n, which is optimized for speed on modest hardware. If you need more accuracy later, you can try larger variants (e.g., YOLO11s/m), understanding they trade FPS for mAP. Begin with 10–20 epochs for small/medium datasets, enable early stopping (patience 5–10) to halt when validation stops improving, and resume from your last/best checkpoint instead of restarting from scratch. Set imgsz=640 for a strong accuracy baseline; switch to imgsz=416 or 320 when you need higher FPS and faster training, accepting some loss in small object recall and localization. Keep train and inference resolutions consistent when you want stable accuracy comparisons; if you train at 640 but infer at 320 for speed, measure the impact on your validation/video clips and choose what best fits your use case. Increase batch size as VRAM allows to speed up epochs on GPU; on CPU only systems like ours , keep batch modest to avoid memory thrash. Use device=0 (or “mps” on Apple Silicon) to leverage the GPU and enable mixed precision if supported; this often yields 2–4× speedups over full precision. After each run, select the best model at runs/detect/trainX/weights/best.pt and update the app’s model path. For deployment speed without retraining, you can also lower imgsz and raise conf during inference to reduce per frame compute and post processing.
